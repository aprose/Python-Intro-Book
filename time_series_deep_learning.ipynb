{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Analysis with PyTorch\n",
    "\n",
    "This notebook demonstrates how to use PyTorch for time series forecasting using LSTM (Long Short-Term Memory) networks with NVIDIA CUDA GPU acceleration support.\n",
    "\n",
    "## Topics Covered:\n",
    "- NVIDIA CUDA/GPU detection and setup\n",
    "- Generating dummy time series data\n",
    "- Building an LSTM model for time series prediction\n",
    "- Training the model\n",
    "- Making predictions\n",
    "- Visualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NVIDIA CUDA/GPU Setup\n",
    "\n",
    "Check if NVIDIA CUDA is available and set the device accordingly for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Dummy Time Series Data\n",
    "\n",
    "We'll create synthetic time series data with a trend, seasonality, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(n_samples=1000, seq_length=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data with trend and seasonality.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of data points\n",
    "        seq_length: Length of each sequence for training\n",
    "    \n",
    "    Returns:\n",
    "        data: Generated time series array\n",
    "    \"\"\"\n",
    "    time = np.arange(n_samples)\n",
    "    \n",
    "    # Trend component\n",
    "    trend = 0.02 * time\n",
    "    \n",
    "    # Seasonal component (daily and weekly patterns)\n",
    "    seasonal_daily = 10 * np.sin(2 * np.pi * time / 24)\n",
    "    seasonal_weekly = 5 * np.sin(2 * np.pi * time / 168)\n",
    "    \n",
    "    # Noise\n",
    "    noise = np.random.normal(0, 2, n_samples)\n",
    "    \n",
    "    # Combine all components\n",
    "    data = trend + seasonal_daily + seasonal_weekly + noise + 50\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "data = generate_time_series(n_samples=2000)\n",
    "\n",
    "# Visualize the generated time series\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(data[:500])\n",
    "plt.title('Generated Time Series Data (First 500 Points)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data range: [{data.min():.2f}, {data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "\n",
    "Create sequences and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for time series data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence and next value\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor([y])\n",
    "\n",
    "# Normalize data\n",
    "data_mean = data.mean()\n",
    "data_std = data.std()\n",
    "data_normalized = (data - data_mean) / data_std\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(data_normalized) * 0.8)\n",
    "train_data = data_normalized[:train_size]\n",
    "test_data = data_normalized[train_size:]\n",
    "\n",
    "# Create datasets\n",
    "seq_length = 50\n",
    "train_dataset = TimeSeriesDataset(train_data, seq_length)\n",
    "test_dataset = TimeSeriesDataset(test_data, seq_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define LSTM Model\n",
    "\n",
    "Build a neural network with LSTM layers for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based model for time series forecasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input to [batch_size, seq_length, input_size]\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        # lstm_out shape: [batch_size, seq_length, hidden_size]\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use the last output for prediction\n",
    "        # Shape: [batch_size, hidden_size]\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        prediction = self.fc(last_output)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# Create model instance\n",
    "model = LSTMModel(input_size=1, hidden_size=64, num_layers=2, output_size=1).to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup\n",
    "\n",
    "Define loss function, optimizer, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Train the LSTM model on the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Train the LSTM model.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Move data to device\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device)\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Test Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, initial_sequence, n_steps, device):\n",
    "    \"\"\"\n",
    "    Predict future values using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        initial_sequence: Initial sequence to start predictions\n",
    "        n_steps: Number of future steps to predict\n",
    "        device: Device to run predictions on\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Array of predicted values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    current_sequence = torch.FloatTensor(initial_sequence).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            # Predict next value\n",
    "            pred = model(current_sequence)\n",
    "            predictions.append(pred.cpu().item())\n",
    "            \n",
    "            # Update sequence with prediction\n",
    "            current_sequence = torch.cat([current_sequence[:, 1:], pred.unsqueeze(1)], dim=1)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get initial sequence from test data\n",
    "initial_seq = test_data[:seq_length]\n",
    "n_future_steps = 100\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_future(model, initial_seq, n_future_steps, device)\n",
    "\n",
    "# Denormalize predictions and actual data\n",
    "predictions_denorm = predictions * data_std + data_mean\n",
    "actual_denorm = test_data[seq_length:seq_length + n_future_steps] * data_std + data_mean\n",
    "\n",
    "print(f\"Generated {n_future_steps} future predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions vs Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(range(len(actual_denorm)), actual_denorm, label='Actual', linewidth=2)\n",
    "plt.plot(range(len(predictions_denorm)), predictions_denorm, label='Predicted', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Prediction: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate prediction accuracy metrics\n",
    "mse = np.mean((actual_denorm - predictions_denorm) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(actual_denorm - predictions_denorm))\n",
    "\n",
    "print(f\"\\nPrediction Metrics:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on entire test set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        all_predictions.extend(outputs.cpu().numpy())\n",
    "        all_actuals.extend(batch_y.numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "all_actuals = np.array(all_actuals).flatten()\n",
    "\n",
    "# Denormalize\n",
    "all_predictions_denorm = all_predictions * data_std + data_mean\n",
    "all_actuals_denorm = all_actuals * data_std + data_mean\n",
    "\n",
    "# Calculate metrics\n",
    "final_mse = np.mean((all_actuals_denorm - all_predictions_denorm) ** 2)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = np.mean(np.abs(all_actuals_denorm - all_predictions_denorm))\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Device Used: {device}\")\n",
    "print(f\"Model Architecture: LSTM with {model.num_layers} layers\")\n",
    "print(f\"Hidden Size: {model.hidden_size}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Testing Samples: {len(test_dataset)}\")\n",
    "print(f\"\\nFinal Metrics on Test Set:\")\n",
    "print(f\"  MSE:  {final_mse:.4f}\")\n",
    "print(f\"  RMSE: {final_rmse:.4f}\")\n",
    "print(f\"  MAE:  {final_mae:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save the Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "model_path = 'lstm_time_series_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'data_mean': data_mean,\n",
    "    'data_std': data_std,\n",
    "    'seq_length': seq_length\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# To load the model later:\n",
    "# checkpoint = torch.load(model_path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. \u2705 CUDA/GPU detection and configuration for PyTorch\n",
    "2. \u2705 Generation of synthetic time series data with trend and seasonality\n",
    "3. \u2705 Building an LSTM neural network for time series forecasting\n",
    "4. \u2705 Training the model with proper data loading and batching\n",
    "5. \u2705 Making predictions on future time steps\n",
    "6. \u2705 Visualizing results and evaluating model performance\n",
    "7. \u2705 Saving and loading model checkpoints\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different model architectures (GRU, Transformer)\n",
    "- Try different hyperparameters (hidden size, number of layers, learning rate)\n",
    "- Use real-world time series data\n",
    "- Implement multi-step ahead forecasting\n",
    "- Add attention mechanisms for improved performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}